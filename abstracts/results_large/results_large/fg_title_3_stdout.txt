Reading train set: data\train_jan_june_2018\train_data_jan_june_2018_processed.csv
Reading test set: data\test\test_data_jan_2018_processed.csv
Normalize training abstracts
Normalize test abstracts
Generate one hot outputs in training set
Generate one hot outputs in test set
77349 documents in train set
19598 documents in test set
Extracting features
Total feature count: 21575
Extracting 100.0% best features by a chi-squared test
Train binary relevance logistic regression tagger
#
# TEST SET
#
accuracy score: 0.7328298806000613

classification report:
              precision    recall  f1-score   support

          cs       0.86      0.81      0.84      5134
        eess       0.66      0.16      0.26       413
        math       0.85      0.82      0.84      6255
     physics       0.94      0.90      0.92      9568
       q-bio       0.65      0.23      0.34       350
       q-fin       0.79      0.37      0.51       131
        stat       0.73      0.44      0.55      1837

   micro avg       0.88      0.80      0.84     23688
   macro avg       0.78      0.53      0.61     23688
weighted avg       0.87      0.80      0.83     23688
 samples avg       0.87      0.85      0.84     23688


classifier details:
best_params = {'classifier': LogisticRegression(C=10, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='warn',
          n_jobs=None, penalty='l2', random_state=None, solver='liblinear',
          tol=0.0001, verbose=0, warm_start=False), 'classifier__C': 10}
#
# TRAINING SET
#
accuracy score: 0.8482204036251276

classification report:
              precision    recall  f1-score   support

          cs       0.93      0.90      0.92     20329
        eess       0.92      0.45      0.61      1373
        math       0.93      0.90      0.92     25658
     physics       0.97      0.95      0.96     36409
       q-bio       0.96      0.59      0.73      1506
       q-fin       0.96      0.65      0.77       570
        stat       0.88      0.67      0.76      5504

   micro avg       0.95      0.89      0.92     91349
   macro avg       0.94      0.73      0.81     91349
weighted avg       0.95      0.89      0.92     91349
 samples avg       0.94      0.92      0.92     91349


classifier details:
best_params = {'classifier': LogisticRegression(C=10, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='warn',
          n_jobs=None, penalty='l2', random_state=None, solver='liblinear',
          tol=0.0001, verbose=0, warm_start=False), 'classifier__C': 10}



=== summary ===
best_feature_count = 21575
best_feature_ratio = 1.0%

#
# TEST SET
#
accuracy score: 0.7328298806000613

classification report:
              precision    recall  f1-score   support

          cs       0.86      0.81      0.84      5134
        eess       0.66      0.16      0.26       413
        math       0.85      0.82      0.84      6255
     physics       0.94      0.90      0.92      9568
       q-bio       0.65      0.23      0.34       350
       q-fin       0.79      0.37      0.51       131
        stat       0.73      0.44      0.55      1837

   micro avg       0.88      0.80      0.84     23688
   macro avg       0.78      0.53      0.61     23688
weighted avg       0.87      0.80      0.83     23688
 samples avg       0.87      0.85      0.84     23688


classifier details:
best_params = {'classifier': LogisticRegression(C=10, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='warn',
          n_jobs=None, penalty='l2', random_state=None, solver='liblinear',
          tol=0.0001, verbose=0, warm_start=False), 'classifier__C': 10}
best_feature_count = 21575
best_feature_ratio = 1.0%

#
# TRAINING SET
#
accuracy score: 0.8482204036251276

classification report:
              precision    recall  f1-score   support

          cs       0.93      0.90      0.92     20329
        eess       0.92      0.45      0.61      1373
        math       0.93      0.90      0.92     25658
     physics       0.97      0.95      0.96     36409
       q-bio       0.96      0.59      0.73      1506
       q-fin       0.96      0.65      0.77       570
        stat       0.88      0.67      0.76      5504

   micro avg       0.95      0.89      0.92     91349
   macro avg       0.94      0.73      0.81     91349
weighted avg       0.95      0.89      0.92     91349
 samples avg       0.94      0.92      0.92     91349


classifier details:
best_params = {'classifier': LogisticRegression(C=10, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='warn',
          n_jobs=None, penalty='l2', random_state=None, solver='liblinear',
          tol=0.0001, verbose=0, warm_start=False), 'classifier__C': 10}

Reading train set: data\train_old\train_data_old_set_processed.csv
Reading test set: data\test_old\test_data_old_set_processed.csv
Normalize training abstracts
Normalize test abstracts
Generate one hot outputs in training set
Generate one hot outputs in test set
9665 documents in train set
3522 documents in test set
Extracting features
Total feature count: 4215
Extracting 100.0% best features by a chi-squared test
Train binary relevance logistic regression tagger
#
# TEST SET
#
accuracy score: 0.6922203293583191

classification report:
              precision    recall  f1-score   support

          cs       0.88      0.79      0.83      1060
        eess       0.86      0.49      0.62        90
        math       0.85      0.83      0.84      1371
     physics       0.90      0.86      0.88      1590
       q-bio       0.91      0.43      0.59        90
       q-fin       0.92      0.32      0.48        37
        stat       0.79      0.59      0.67       327

   micro avg       0.87      0.80      0.83      4565
   macro avg       0.87      0.62      0.70      4565
weighted avg       0.87      0.80      0.83      4565
 samples avg       0.83      0.81      0.81      4565


classifier details:
best_params = {'classifier': LogisticRegression(C=10, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='warn',
          n_jobs=None, penalty='l2', random_state=None, solver='liblinear',
          tol=0.0001, verbose=0, warm_start=False), 'classifier__C': 10}
#
# TRAINING SET
#
accuracy score: 0.8602172788411795

classification report:
              precision    recall  f1-score   support

          cs       0.95      0.90      0.92      2532
        eess       0.98      0.60      0.74       217
        math       0.95      0.91      0.93      3447
     physics       0.98      0.95      0.96      4525
       q-bio       0.99      0.57      0.73       208
       q-fin       1.00      0.55      0.71        83
        stat       0.95      0.70      0.81       770

   micro avg       0.96      0.90      0.93     11782
   macro avg       0.97      0.74      0.83     11782
weighted avg       0.96      0.90      0.93     11782
 samples avg       0.95      0.92      0.93     11782


classifier details:
best_params = {'classifier': LogisticRegression(C=10, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='warn',
          n_jobs=None, penalty='l2', random_state=None, solver='liblinear',
          tol=0.0001, verbose=0, warm_start=False), 'classifier__C': 10}



=== summary ===
best_feature_count = 4215
best_feature_ratio = 1.0%

#
# TEST SET
#
accuracy score: 0.6922203293583191

classification report:
              precision    recall  f1-score   support

          cs       0.88      0.79      0.83      1060
        eess       0.86      0.49      0.62        90
        math       0.85      0.83      0.84      1371
     physics       0.90      0.86      0.88      1590
       q-bio       0.91      0.43      0.59        90
       q-fin       0.92      0.32      0.48        37
        stat       0.79      0.59      0.67       327

   micro avg       0.87      0.80      0.83      4565
   macro avg       0.87      0.62      0.70      4565
weighted avg       0.87      0.80      0.83      4565
 samples avg       0.83      0.81      0.81      4565


classifier details:
best_params = {'classifier': LogisticRegression(C=10, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='warn',
          n_jobs=None, penalty='l2', random_state=None, solver='liblinear',
          tol=0.0001, verbose=0, warm_start=False), 'classifier__C': 10}
best_feature_count = 4215
best_feature_ratio = 1.0%

#
# TRAINING SET
#
accuracy score: 0.8602172788411795

classification report:
              precision    recall  f1-score   support

          cs       0.95      0.90      0.92      2532
        eess       0.98      0.60      0.74       217
        math       0.95      0.91      0.93      3447
     physics       0.98      0.95      0.96      4525
       q-bio       0.99      0.57      0.73       208
       q-fin       1.00      0.55      0.71        83
        stat       0.95      0.70      0.81       770

   micro avg       0.96      0.90      0.93     11782
   macro avg       0.97      0.74      0.83     11782
weighted avg       0.96      0.90      0.93     11782
 samples avg       0.95      0.92      0.93     11782


classifier details:
best_params = {'classifier': LogisticRegression(C=10, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='warn',
          n_jobs=None, penalty='l2', random_state=None, solver='liblinear',
          tol=0.0001, verbose=0, warm_start=False), 'classifier__C': 10}
